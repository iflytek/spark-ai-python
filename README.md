# è®¯é£æ˜Ÿç«å¤§æ¨¡å‹æ¥å…¥åº“ (spark-ai-python)


<!-- markdownlint-disable MD033 -->
<span class="badge-placeholder">[![Stars](https://img.shields.io/github/stars/iflytek/spark-ai-python)](https://img.shields.io/github/forks/iflytek/spark-ai-python)</span>
<span class="badge-placeholder">[![Forks](https://img.shields.io/github/forks/iflytek/spark-ai-python)](https://img.shields.io/github/forks/iflytek/spark-ai-python)</span>
<span class="badge-placeholder">[![GitHub release](https://img.shields.io/github/v/release/iflytek/spark-ai-python)](https://github.com/iflytek/spark-ai-python/releases/latest)</span>
<span class="badge-placeholder">[![GitHub contributors](https://img.shields.io/github/contributors/xfyun/AthenaServing)](https://github.com/xfyun/AthenaServing/graphs/contributors)</span>
<span class="badge-placeholder">[![License: Apache2.0](https://img.shields.io/github/license/iflytek/spark-ai-python)](https://github.com/iflytek/aiges/blob/master/LICENSE)</span>


æœ¬Python SDKåº“å¸®åŠ©ç”¨æˆ·æ›´å¿«ä½“éªŒè®¯é£æ˜Ÿç«å¤§æ¨¡å‹ï¼Œæ›´ç®€å•ï¼Œæ›´å€¼å¾—ä¾èµ–


## é¡¹ç›®åœ°å€

* Github: [https://github.com/iflytek/spark-ai-python](https://github.com/iflytek/spark-ai-python) æ¬¢è¿ç‚¹èµ [Star]()


## å‰è¨€

é•¿ä¹…ä»¥æ¥ï¼Œpythonæ¥å…¥æ˜Ÿç«å¤§æ¨¡å‹æ²¡æœ‰ä¸€ä¸ªç»Ÿä¸€å®˜æ–¹ç»´æŠ¤çš„Libraryï¼Œ
æ­¤ç•ªå¼€æºæœ¬sdkï¼Œä¹Ÿæ˜¯ä¸ºäº†èƒ½å¤Ÿè®©æ˜Ÿç«å¤§æ¨¡å‹æ›´å¿«è½åˆ°å®é™…çš„ä¸€äº›AIå¤§æ¨¡å‹åº”ç”¨ç›¸å…³çš„å¼€å‘ä»»åŠ¡ä¸­å»ï¼Œç®€åŒ–pythonç”¨æˆ·è°ƒç”¨å¤§æ¨¡å‹æˆæœ¬ã€‚

ç›®å‰åŸºäºLangchainçš„ä¸€äº›åŸºç¡€æ•°æ®ç±»å‹ç§»æ¤å¼€å‘å¾—åˆ°æœ¬é¡¹ç›®ï¼Œéƒ¨åˆ†æ ¸å¿ƒå®ç°å¦‚æœ‰é›·åŒï¼Œçº¯å±"å­¦ä¹ "ï¼
æ„Ÿè°¢å¼€æºçš„åŠ›é‡ï¼Œå¸Œæœ›è®¯é£å¼€æºè¶Šåšè¶Šå¥½ï¼Œæ˜Ÿç«å¤§æ¨¡å‹æ•ˆæœè¶Šæ¥è¶Šå¥½ï¼ã€‚


![!img](log.jpg)

**æœ¬logoå‡ºè‡ª[æ˜Ÿç«å¤§æ¨¡å‹](https://xinghuo.xfyun.cn/)**

***æ„Ÿè°¢ç¤¾åŒº(Langchainé¡¹ç›®ä»¥åŠSparkLLMéƒ¨åˆ†committer)[é¡¹ç›®æ­£åœ¨å¼€å‘ä¸­]***


## æ–°ç‰¹æ€§ï¼ï¼ğŸ‘‰ğŸ‘‰ğŸ‘‰ç”Ÿæ€å¯¹æ¥

- [x] æ”¯æŒLLamaIndex,è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [LLamIndex Support](#llama_index)
- [x] æ”¯æŒAutoGen,è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [AutoGen Support](#autogen)
- [x] æ”¯æŒæ˜Ÿç«å›¾ç‰‡ç†è§£å¤§æ¨¡å‹,è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [ImageUnderstanding Support](#imageunderstanding)

## è¿‘æœŸè§„åˆ’æ–°ç‰¹æ€§[å¾…æ¼”è¿›]

- [x] å¼€æºæ¡†æ¶AutoGPT/AutoGen/MetaGpt/Langchain/PromptFlow/.... å¿«é€Ÿé›†æˆæ˜Ÿç«ç¤ºä¾‹
- [x] æç®€çš„æ¥å…¥,å¿«é€Ÿè°ƒç”¨è®¯é£æ˜Ÿç«å¤§æ¨¡å‹
- [x] å·²å‘å¸ƒpypi [å›½å†…æºå‡å¯å®‰è£…]
- [x] æœ¬åœ°ä»£ç†æ–¹å¼æ˜Ÿç«SparkAPIè½¬OpenAIæ¥å£(è®©ä½ å¿«é€Ÿåœ¨å¼€æºagentæ¡†æ¶é›†æˆæ˜Ÿç«å¤§æ¨¡å‹) 
- [x] æ— ç¼å¯¹æ¥[è®¯é£Maaså¹³å°](https://training.xfyun.cn/)å¾®è°ƒè®­ç»ƒæ‰˜ç®¡çš„å¤§æ¨¡å‹API
- [ ] SDKæ–¹å¼é€‚é…OpenAIæ¥å£ ChatCompletionæ¥å£ 
- [x] SDKæ–¹å¼é€‚é…OpenAI Embeddingæ¥å£ [å®éªŒæ€§æ”¯æŒ](#embeddingæ”¯æŒ)
- [ ] æ”¯æŒ HTTP SPARK API
- [x] æ”¯æŒå¤§æ¨¡å‹å¤šæ¨¡æ€ç­‰èƒ½åŠ› (ç›®å‰å·²æ”¯æŒå›¾ç‰‡ç†è§£å¤§æ¨¡å‹)
- [ ] Golangç‰ˆæœ¬[SDK](https://github.com/iflytek/spark-ai-go/)è¿›è¡Œä¸­
- [ ] å¯¹æ¥ [liteLLM](https://github.com/BerriAI/litellm)
- [x] æ–°å¢ streamæ¥å£æ”¯æŒ



## å®‰è£…

**é¡¹ç›®ä»…æ”¯æŒ Python3.8+**

å¦‚æœä½ ä¸éœ€è¦æºç ï¼Œåªéœ€è¦é€šè¿‡ `pip `å¿«é€Ÿå®‰è£…

```sh
pip install --upgrade spark_ai_python
```
å›½å†…ä½¿ç”¨:
```bash
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple spark_ai_python
```

### Upgrade

å¦‚æœæ¸…åæºç‰ˆæœ¬ä¸å¯ç”¨,è¯·ä½¿ç”¨ä¸€ä¸‹å‘½ä»¤å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬:

```sh
pip install -i  https://repo.model.xfyun.cn/api/packages/administrator/pypi/simple  spark_ai_python --upgrade

```

Install from source with:

```sh
pip install -e .
```

## å¦‚ä½•ä½¿ç”¨

### ç¤ºä¾‹ä»£ç 

* å‰ç½®æ¡ä»¶
  éœ€è¦åœ¨ xfyun.cn ç”³è¯·æœ‰æƒé™çš„
  * app_id
  * api_key
  * api_secret

* URL/Domainé…ç½®è¯·æŸ¥çœ‹[doc](https://www.xfyun.cn/doc/spark/Web.html#_1-%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E)
* è¿è¡Œæµ‹è¯•è„šæœ¬éœ€è¦æå‰å°† [.env.example](.env.example) æ‹·è´ä¸º `.env`å¹¶é…ç½®å…¶ä¸­å˜é‡

### ä¸€æ¬¡æ€§è¿”å›ç»“æœ(éæµå¼)

tests/examples/llm_test.py

```python
import os

from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler
from sparkai.core.messages import ChatMessage
try:
    from dotenv import load_dotenv
except ImportError:
    raise RuntimeError('Python environment for SPARK AI is not completely set up: required package "python-dotenv" is missing.') from None

load_dotenv()

if __name__ == '__main__':
    from sparkai.core.callbacks import StdOutCallbackHandler
    spark = ChatSparkLLM(
        spark_api_url=os.environ["SPARKAI_URL"],
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain=os.environ["SPARKAI_DOMAIN"],
        streaming=False,
    )
    messages = [ChatMessage(
        role="user",
        content='ä½ å¥½å‘€'
    )]
    handler = ChunkPrintHandler()
    a = spark.generate([messages], callbacks=[handler])
    print(a)
    print(a.generations[0][0].text)
    print(a.llm_output)
```
æ³¨æ„å½“`streaming`è®¾ç½®ä¸º `False`çš„æ—¶å€™, callbacks å¹¶ä¸èµ·ä½œç”¨ã€‚

### æµå¼è¿”å›ç»“æœ

#### æ–¹å¼1: å›è°ƒå‡½æ•°
tests/examples/llm_test.py

```python
import os

from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler
from sparkai.core.messages import ChatMessage
try:
    from dotenv import load_dotenv
except ImportError:
    raise RuntimeError('Python environment for SPARK AI is not completely set up: required package "python-dotenv" is missing.') from None

load_dotenv()
def test_stream():
    from sparkai.core.callbacks import StdOutCallbackHandler
    spark = ChatSparkLLM(
        spark_api_url=os.environ["SPARKAI_URL"],
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain=os.environ["SPARKAI_DOMAIN"],
        request_timeout=30, # 
        streaming=True,

    )
    messages = [ChatMessage(
        role="user",
        content='ä½œä¸ºAutoSparkçš„åˆ›å»ºè€…è§’è‰²ï¼Œå½“å‰éœ€è¦ä½ å¸®æˆ‘åˆ†æå¹¶ç”Ÿæˆä»»åŠ¡ï¼Œä½ å½“å‰ä¸»è¦ç›®æ ‡æ˜¯:\n1. å¸®æˆ‘å†™ä¸ªè´ªåƒè›‡pythonæ¸¸æˆ\n\n\n\n\nå½“å‰æ‰§è¡Œä»»åŠ¡èŠ‚ç‚¹æ˜¯: `ç¼–å†™è´ªåƒè›‡æ¸¸æˆçš„ç•Œé¢è®¾è®¡`\n\nä»»åŠ¡æ‰§è¡Œå†å²æ˜¯:\n`\nTask: ä½¿ç”¨ThinkingToolåˆ†æè´ªåƒè›‡æ¸¸æˆçš„éœ€æ±‚\nResult: Error2: {\'error\': \'Could not parse invalid format: æ ¹æ®ä»»åŠ¡èŠ‚ç‚¹ï¼Œæˆ‘å°†ä½¿ç”¨ThinkingToolæ¥åˆ†æè´ªåƒè›‡æ¸¸æˆçš„éœ€æ±‚ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç†è§£æ¸¸æˆçš„åŸºæœ¬åŠŸèƒ½å’Œè§„åˆ™ã€‚è´ªåƒè›‡æ¸¸æˆçš„ä¸»è¦ç›®æ ‡æ˜¯æ§åˆ¶ä¸€æ¡è›‡åœ¨å±å¹•ä¸Šç§»åŠ¨ï¼Œåƒåˆ°é£Ÿç‰©åè›‡ä¼šå˜é•¿ï¼Œç¢°åˆ°è‡ªå·±çš„èº«ä½“æˆ–è€…å±å¹•è¾¹ç¼˜åˆ™æ¸¸æˆç»“æŸã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨CodingToolæ¥ç¼–å†™è´ªåƒè›‡æ¸¸æˆçš„ä»£ç ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä»¥ä¸‹æ ¸å¿ƒç±»å’Œæ–¹æ³•ï¼š\\n\\n1. Snakeç±»ï¼šç”¨äºè¡¨ç¤ºè´ªåƒè›‡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬è›‡çš„èº«ä½“ã€ç§»åŠ¨æ–¹å‘ç­‰ã€‚\\n2. Foodç±»ï¼šç”¨äºè¡¨ç¤ºé£Ÿç‰©çš„ä½ç½®ã€‚\\n3. Gameç±»ï¼šç”¨äºæ§åˆ¶æ¸¸æˆçš„è¿›è¡Œï¼ŒåŒ…æ‹¬åˆå§‹åŒ–æ¸¸æˆã€æ›´æ–°è›‡çš„ä½ç½®ã€æ£€æŸ¥ç¢°æ’ç­‰ã€‚\\n4. mainå‡½æ•°ï¼šç”¨äºå¯åŠ¨æ¸¸æˆã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›ç±»å’Œæ–¹æ³•çš„ä»£ç å†™å…¥æ–‡ä»¶ä¸­ã€‚\\n\\næœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨WriteTestToolæ¥ç¼–å†™æµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿æˆ‘ä»¬çš„ä»£ç èƒ½å¤Ÿæ­£ç¡®åœ°è¿è¡Œã€‚æµ‹è¯•ç”¨ä¾‹åº”è¯¥åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\\n\\n1. æµ‹è¯•æ¸¸æˆæ˜¯å¦èƒ½æ­£ç¡®åˆå§‹åŒ–ã€‚\\n2. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®ç§»åŠ¨ã€‚\\n3. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®åƒåˆ°é£Ÿç‰©å¹¶å˜é•¿ã€‚\\n4. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®ç¢°åˆ°è‡ªå·±çš„èº«ä½“æˆ–å±å¹•è¾¹ç¼˜å¯¼è‡´æ¸¸æˆç»“æŸã€‚ exceptionNot get command from llm response...\'}. \nTask: ç¼–å†™è´ªåƒè›‡æ¸¸æˆçš„specæ–‡ä»¶\nResult: Error2: {\'error\': \'Could not parse invalid format: æ ¹æ®ä»»åŠ¡èŠ‚ç‚¹ï¼Œæˆ‘å°†ä½¿ç”¨`WriteSpecTool`æ¥ç¼–å†™è´ªåƒè›‡æ¸¸æˆçš„specæ–‡ä»¶ã€‚\\n\\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä»¥ä¸‹æ ¸å¿ƒç±»å’Œæ–¹æ³•ï¼š\\n1. Snakeç±»ï¼šç”¨äºè¡¨ç¤ºè´ªåƒè›‡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬è›‡çš„èº«ä½“ã€ç§»åŠ¨æ–¹å‘ç­‰ã€‚\\n2. Foodç±»ï¼šç”¨äºè¡¨ç¤ºé£Ÿç‰©çš„ä½ç½®ã€‚\\n3. Gameç±»ï¼šç”¨äºæ§åˆ¶æ¸¸æˆçš„è¿›è¡Œï¼ŒåŒ…æ‹¬åˆå§‹åŒ–æ¸¸æˆã€æ›´æ–°è›‡çš„ä½ç½®ã€æ£€æŸ¥ç¢°æ’ç­‰ã€‚\\n4. mainå‡½æ•°ï¼šç”¨äºå¯åŠ¨æ¸¸æˆã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›ç±»å’Œæ–¹æ³•çš„ä»£ç å†™å…¥æ–‡ä»¶ä¸­ã€‚\\n\\næœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`WriteTestTool`æ¥ç¼–å†™æµ‹è¯•ç”¨ä¾‹ï¼Œç¡®ä¿æˆ‘ä»¬çš„ä»£ç èƒ½å¤Ÿæ­£ç¡®åœ°è¿è¡Œã€‚æµ‹è¯•ç”¨ä¾‹åº”è¯¥åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\\n1. æµ‹è¯•æ¸¸æˆæ˜¯å¦èƒ½æ­£ç¡®åˆå§‹åŒ–ã€‚\\n2. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®ç§»åŠ¨ã€‚\\n3. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®åƒåˆ°é£Ÿç‰©å¹¶å˜é•¿ã€‚\\n4. æµ‹è¯•è›‡æ˜¯å¦èƒ½æ­£ç¡®ç¢°åˆ°è‡ªå·±çš„èº«ä½“æˆ–å±å¹•è¾¹ç¼˜å¯¼è‡´æ¸¸æˆç»“æŸã€‚ exceptionNot get command from llm response...\'}. \n\n`\n\næ ¹æ®ä¸Šè¿°èƒŒæ™¯ä¿¡æ¯ï¼Œä½ çš„ä»»åŠ¡æ˜¯éœ€è¦ç†è§£å½“å‰çš„ä»»åŠ¡èŠ‚ç‚¹å…³é”®ä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªè§„åˆ’ï¼Œè§£é‡Šä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Œå¹¶ä¸”æåŠä¸€äº›éœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼Œå¿…é¡»ä»ä¸‹è¿°TOOLSä¸­æŒ‘é€‰ä¸€ä¸ªå‘½ä»¤ç”¨äºä¸‹ä¸€æ­¥æ‰§è¡Œã€‚\n\nTOOLS:\n1. "ThinkingTool": Intelligent problem-solving assistant that comprehends tasks, identifies key variables, and makes efficient decisions, all while providing detailed, self-driven reasoning for its choices. Do not assume anything, take the details from given data only., args : task_description: "<task_description>",\n2. "WriteSpecTool": A tool to write the spec of a program., args : task_description: "<task_description>",spec_file_name: "<spec_file_name>",\n3. "CodingTool": You will get instructions for code to write. You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code. Think step by step and reason yourself to the right decisions to make sure we get it right. You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose. Then you will output the content of each file including ALL code., args : code_description: "<code_description>",\n4. "WriteTestTool": æ‚¨æ˜¯ä¸€ä½è¶…çº§èªæ˜çš„å¼€å‘äººå‘˜ï¼Œä½¿ç”¨æµ‹è¯•é©±åŠ¨å¼€å‘æ ¹æ®è§„èŒƒç¼–å†™æµ‹è¯•ã€‚\nè¯·æ ¹æ®ä¸Šè¿°è§„èŒƒç”Ÿæˆæµ‹è¯•ã€‚æµ‹è¯•åº”è¯¥å°½å¯èƒ½ç®€å•ï¼Œ ä½†ä»ç„¶æ¶µç›–äº†æ‰€æœ‰åŠŸèƒ½ã€‚\nå°†å®ƒä»¬å†™å…¥æ–‡ä»¶ä¸­, args : test_description: "<test_description>",test_file_name: "<test_file_name>",\n\n\n\nçº¦æŸæ¡ä»¶:\n1. è¯·æ³¨æ„è¿”å›çš„å‘½ä»¤åç§°å’Œå‚æ•°ä¸è¦è¢«å¼•å·åŒ…è£¹\n2. å‘½ä»¤åç§°å¿…é¡»æ˜¯TOOLSä¸­çš„å·²çŸ¥çš„\n3. ä½ åªèƒ½ç”Ÿæˆä¸€ä¸ªå¾…æ‰§è¡Œå‘½ä»¤åç§°åŠå…¶å¯¹åº”å‚æ•°\n4. ä½ ç”Ÿæˆçš„å‘½ä»¤å¿…é¡»æ˜¯ç”¨æ¥è§£å†³ `ç¼–å†™è´ªåƒè›‡æ¸¸æˆçš„ç•Œé¢è®¾è®¡`\n\nåœ¨ä¹‹åçš„æ¯æ¬¡å›ç­”ä¸­ï¼Œä½ å¿…é¡»ä¸¥æ ¼éµä»ä¸Šè¿°çº¦æŸæ¡ä»¶å¹¶æŒ‰ç…§å¦‚ä¸‹JsonSchemaçº¦æŸè¿”å›å“åº”:\n\n{\n "$schema": "http://json-schema.org/draft-07/schema#",\n "type": "object",\n "properties": {\n "thoughts": {\n "type": "object",\n "properties": {\n "reasoning": {\n "type": "string",\n "description": "short reasoning",\n }\n },\n "required": ["reasoning"]\n },\n "tool": {\n "type": "object",\n "properties": {\n "name": {\n "type": "string",\n "description": "tool name",\n },\n "args": {\n "type": "object",\n "description": "tool arguments",\n }\n },\n "required": ["name", "args"]\n }\n }\n}'
    )]
    handler = ChunkPrintHandler()
    a = spark.generate([messages], callbacks=[handler])
    print(a)
```

å…¶ä¸­ `ChunkPrintHandler` ä¸ºå›è°ƒç±»ï¼Œå¯ä»¥åœ¨å›è°ƒç±»å¤„ç†æµå¼å“åº”çš„chunkï¼Œ
è¯¥ç±»ç®€å•å®ç°å¦‚ä¸‹:

```python
class ChunkPrintHandler(BaseCallbackHandler):
    """Callback Handler that prints to std out."""

    def __init__(self, color: Optional[str] = None) -> None:
        """Initialize callback handler."""
        self.color = color

    def on_llm_new_token(self,  token: str,
        *,
        chunk:  None,
        **kwargs: Any,):
        print(token)
        print(kwargs)
        # token ä¸ºæ¨¡å‹ç”Ÿæˆçš„token
        # å¯ä»¥check kwargså†…å®¹ï¼Œkwargsä¸­ llm_outputä¸­æœ‰usageç›¸å…³ä¿¡æ¯ï¼Œ finalè¡¨ç¤ºæ˜¯å¦æ˜¯æœ€åä¸€å¸§
        

```
ä¸Šè¿°åœ¨ on_llm_new_token å®ç°æ‚¨çš„æµå¼å¤„ç†é€»è¾‘,å¦‚éœ€å®šåˆ¶æµå¼å¤„ç†é€»è¾‘ï¼Œè¯·å‚è€ƒä¸Šè¿°å®ç°ï¼Œç»§æ‰¿: BaseCallbackHandler

#### æ–¹å¼2: stream/astreamæ¥å£

ç›´æ¥éå†ç”Ÿæˆå™¨(åŒæ­¥streamæ¥å£):
```python

def test_starcoder2():
    from sparkai.log.logger import logger
    #logger.setLevel("debug")
    from sparkai.core.callbacks import StdOutCallbackHandler
    messages = [{'role': 'user',
                 'content': "å¸®æˆ‘ç”Ÿæˆä¸€æ®µä»£ç ï¼Œçˆ¬å–baidu.com"}]
    spark = ChatSparkLLM(
        spark_api_url="wss://xingchen-api.cn-huabei-1.xf-yun.com/v1.1/chat",
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain="xsstarcoder27binst",
        streaming=True,
        max_tokens= 1024,
    )
    messages = [
                ChatMessage(
                        role="user",
                        content=messages[0]['content']

    )]

    a = spark.stream(messages)
    for message in a:
        print(message)
```

æˆ–è€…æ˜¯å¼‚æ­¥astreamæ¥å£:

```python

async def test_astream():
    from sparkai.log.logger import logger
    #logger.setLevel("debug")
    from sparkai.core.callbacks import StdOutCallbackHandler
    messages = [{'role': 'user',
                 'content': "å¸®æˆ‘ç”Ÿæˆä¸€æ®µä»£ç ï¼Œçˆ¬å–baidu.com"}]
    spark = ChatSparkLLM(
        spark_api_url="wss://xingchen-api.cn-huabei-1.xf-yun.com/v1.1/chat",
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain="xsstarcoder27binst",
        streaming=True,
        max_tokens= 1024,
    )
    messages = [
                ChatMessage(
                        role="user",
                        content=messages[0]['content']

    )]
    handler = AsyncChunkPrintHandler()
    a = spark.astream(messages, config={"callbacks": [handler]})
    async for message in a:
        print(message)

if __name__ == '__main__':
    import asyncio
    asyncio.run(test_astream())
```


### FunctionCallåŠŸèƒ½
æ¯”å¦‚å°† mulitply ä¹˜æ³•å‡½æ•°å®šä¹‰ä¼ å…¥ ChatSparkLLM

```python
from sparkai.core.utils.function_calling import convert_to_openai_tool, convert_to_openai_function
def multiply(a,b :int) -> int:
    """ä½ æ˜¯ä¸€ä¸ªä¹˜æ³•è®¡ç®—å™¨ï¼Œå¯ä»¥å¸®æˆ‘è®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯ï¼Œä¾‹å¦‚ï¼šè®¡ç®—1ä¹˜1ç­‰äºå‡ æˆ–è®¡ç®—1*1ç­‰äºå‡ 
    Args:
        a: è¾“å…¥a
        b: è¾“å…¥b
    Return:
         è¿”å› a*b ç»“æœ
    """
    print("hello success")
    return a*b

def test_function_call():
    from sparkai.core.callbacks import StdOutCallbackHandler
    messages = [{'role': 'user',
                 'content': "å¸®æˆ‘ç®—ä¸‹ 12ä¹˜ä»¥12"}]
    spark = ChatSparkLLM(
        spark_api_url=os.environ["SPARKAI_URL"],
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain=os.environ["SPARKAI_DOMAIN"],
        streaming=False,

    )
    function_definition =[convert_to_openai_function(multiply)]
    print(json.dumps(convert_to_openai_tool(multiply),ensure_ascii=False))
    messages = [ChatMessage(
        role="user",
        content=messages[0]['content']

    )]
    handler = ChunkPrintHandler()
    a = spark.generate([messages], callbacks=[handler],function_definition=function_definition)
    print(a)
    print(a.generations[0][0].text) 
    print(a.llm_output)
```
å¾—åˆ°è¾“å‡º

```bash
PASSED                                   [100%]{"type": "function", "function": {"name": "multiply", "description": "ä¹˜æ³•å‡½æ•°ï¼Œ\nArgs:\n    a: è¾“å…¥a\n    b: è¾“å…¥b\nReturn:\n     è¿”å› a*b ç»“æœ", "parameters": {"type": "object", "properties": {"b": {"type": "integer"}}, "required": ["a", "b"]}}}
generations=[[ChatGeneration(message=FunctionCallMessage(content='', function_call={'arguments': '{"a":12,"b":12}', 'name': 'multiply'}))]] llm_output={'token_usage': {'question_tokens': 9, 'prompt_tokens': 9, 'completion_tokens': 0, 'total_tokens': 9}} run=[RunInfo(run_id=UUID('95bf4e2e-6c90-41aa-9ddf-51b707d4d3c7'))]

generations=[[ChatGeneration(message=FunctionCallMessage(content='', function_call={'arguments': '{"a":12,"b":12}', 'name': 'operator_multiply'}))]] llm_output={'token_usage': {'question_tokens': 9, 'prompt_tokens': 9, 'completion_tokens': 0, 'total_tokens': 9}} run=[RunInfo(run_id=UUID('64bb65bd-948b-4354-bb72-e6847cc0a21b'))]

{'token_usage': {'question_tokens': 9, 'prompt_tokens': 9, 'completion_tokens': 0, 'total_tokens': 9}}

```

### æœ¬åœ°ä»£ç†æ–¹å¼æ˜Ÿç«SparkAPIè½¬OpenAIæ¥å£ 

**ä»…ä¾›ç”¨äºè°ƒè¯•æˆ–è€…åº”ç”¨äºä¸‰æ–¹æ¡†æ¶å¿«é€Ÿé›†æˆæ˜Ÿç«**

```python
python -m sparkai.spark_proxy.main 
```
è¿è¡Œåå¦‚ä¸‹:

```bash
INFO:     Started server process [57295]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8008 (Press CTRL+C to quit)

```

ä¹‹åå†éœ€è¦é…ç½®`OPENAI`é…ç½®ä¸Šè¿° æœ¬åœ°url å’Œ æ˜Ÿç« ```key&secret&appid"```ç»„æˆçš„keyå³å¯ä»¥openaiæ¥å£å½¢å¼è°ƒç”¨æ˜Ÿç«å¤§æ¨¡å‹

* open_api_key: é…ç½®æ ¼å¼keyä¸º: ```key&secret&appid"``` æ ¼å¼çš„key
* openai_base_url: ä½ æœ¬åœ° ip:8008ç«¯å£

å…·ä½“æ“ä½œæµç¨‹å‚è§[æœ¬åœ°ä»£ç†æ–¹å¼æ˜Ÿç«SparkAPIè½¬OpenAIæ¥å£ ](tests/examples/docs/proxy_open_ai.md)


## ç”Ÿæ€æ”¯æŒ

<h3 id="llama_index">LLamaIndex Support</h3>
Before using chroma vector store, please install it by running pip install llama-index-vector-stores-chroma.

```python
### çœç•¥å…¶ä»–ä»£ç 
from sparkai.embedding.sparkai_base import SparkAiEmbeddingModel
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext
import chromadb
import os
from sparkai.embedding.spark_embedding import SparkEmbeddingFunction
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from sparkai.frameworks.llama_index import SparkAI

try:
    from dotenv import load_dotenv
except ImportError:
    raise RuntimeError(
        'Python environment for SPARK AI is not completely set up: required package "python-dotenv" is missing.') from None

load_dotenv()


def llama_query():
    chroma_client = chromadb.Client()
    chroma_collection = chroma_client.get_or_create_collection(name="spark")
    # define embedding function
    embed_model = SparkAiEmbeddingModel(spark_embedding_app_id=os.environ['SPARK_Embedding_APP_ID'],
                                        spark_embedding_api_key=os.environ['SPARK_Embedding_API_KEY'],
                                        spark_embedding_api_secret=os.environ['SPARK_Embedding_API_SECRET'],
                                        spark_embedding_domain=os.environ['SPARKAI_Embedding_DOMAIN'],
                                        QPS=2)
    # define LLM Model
    sparkai = SparkAI(
        spark_api_url=os.environ["SPARKAI_URL"],
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain=os.environ["SPARKAI_DOMAIN"],
        streaming=False,
    )
    # load documents
    # Invoke-WebRequest -Uri 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -OutFile 'data\paul_graham\paul_graham_essay.txt'
    documents = SimpleDirectoryReader("D:\data\paul_graham").load_data()
    # set up ChromaVectorStore and load in data
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)
    # query
    query_engine = index.as_query_engine(llm=sparkai, similarity_top_k=2)
    response = query_engine.query("What did the author do growing up?")
    print(response)


if __name__ == "__main__":
    llama_query()


```
* éœ€é…ç½®embedding modelå’ŒLLM modelä¸¤éƒ¨åˆ†ï¼ŒSPARKAI_Embedding_DOMAINä¸ºqueryæˆ–paraã€‚å…·ä½“è¯¦è§https://www.xfyun.cn/doc/spark/Embedding_api.html#_3-%E8%AF%B7%E6%B1%82
* embeddingåŠŸèƒ½ç”³è¯·æˆæƒè¯·å‚è€ƒï¼šhttps://www.xfyun.cn/services/embedding

ç¤ºä¾‹ç»“æœå¦‚ä¸‹:

![img](tests/examples/llama_index_embedding.png)


<h3 id="autogen">AutoGen Support</h3>

å¾®è½¯å‡ºå“çš„[AutoGen](https://github.com/microsoft/autogen)æ˜¯ä¸šç•Œå‡ºåçš„å¤šAgentæ™ºèƒ½ä½“æ¡†æ¶ã€‚
é€šè¿‡å‡ è¡ŒImportå³å¯è®©autogenåŸç”Ÿæ”¯æŒ[ã€æ˜Ÿç«å¤§æ¨¡å‹ã€‘](https://github.com/microsoft/autogen)

```python
from sparkai.frameworks.autogen import SparkAI
import autogen
from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent

spark_config = autogen.config_list_from_json(
    "sparkai_autogen.json",
    filter_dict={"model_client_cls": ["SparkAI"]},
)
llm_config = {
    "timeout": 600,
    "cache_seed": None,  # change the seed for different trials
    "config_list": spark_config,
    "temperature": 0,
}

# 1. create an RetrieveAssistantAgent instance named "assistant"
assistant = RetrieveAssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    llm_config=llm_config
)
# æ³¨å†ŒSparkAIç±»è¿›å…¥ agent
assistant.register_model_client(model_client_cls=SparkAI)

```
å…¶ä¸­`sparkai_autogen.json`å†…å®¹å¦‚ä¸‹:

***å…¶ä¸­æ˜Ÿç«çš„domainå¯¹åº” ä¸‹é¢é…ç½®model***

```json
[
  {
    "api_key": "<spark_api_key>&<spark_api_secret>&<spark_app_id>",
    "base_url": "wss://spark-api.xf-yun.com/v3.5/chat",
    "model_client_cls": "SparkAI",
    "model": "generalv3.5", 
    "stream": true,
    "params": {
      "request_timeout": 61
    }
  }
]


```


<h3 id="imageunderstanding">ImageUnderstanding Support</h3>


æ”¯æŒè®¯é£æ˜Ÿç« [å›¾ç‰‡ç†è§£](https://www.xfyun.cn/doc/spark/ImageUnderstanding.html#%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0)

å¤§æ¨¡å‹:

```python
import base64
image_content = base64.b64encode(open("spark_llama_index.png",'rb').read())

spark = ChatSparkLLM(
    spark_app_id=os.environ["SPARKAI_APP_ID"],
    spark_api_key=os.environ["SPARKAI_API_KEY"],
    spark_api_secret=os.environ["SPARKAI_API_SECRET"],
    spark_llm_domain="image",
    streaming=False,
    user_agent="test"

)
messages = [ImageChatMessage(
    role="user",
    content=image_content,
    content_type="image"
),ImageChatMessage(
    role="user",
    content="è¿™æ˜¯ä»€ä¹ˆå›¾",
    content_type="text"
)]
handler = ChunkPrintHandler()
a = spark.generate([messages], callbacks=[])
```

### Embeddingæ”¯æŒ

å‚è€ƒå¦‚ä¸‹ testæ–¹æ³•:
```python
from sparkai.embedding.spark_embedding import Embeddingmodel, SparkEmbeddingFunction
import chromadb


def test_embedding():
    model = Embeddingmodel(
        spark_app_id="id",
        spark_api_key="key",
        spark_api_secret="secret",
        spark_domain="query",
    )
    # desc = {"messages":[{"content":"cc","role":"user"}]}
    desc = {"content": "cc", "role": "user"}
    # è°ƒç”¨embeddingæ–¹æ³•
    a = model.embedding(text=desc, kind='text')
    # print(len(a))
    print(a)


def test_chroma_embedding():
    chroma_client = chromadb.Client()
    sparkmodel = SparkEmbeddingFunction(
        spark_app_id="id",
        spark_api_key="key",
        spark_api_secret="secret",
        spark_domain="query",
    )
    a = sparkmodel(["This is a document", "This is another document"])
    # print(type(a))
    # print(a[0])
    # print(a[0][1])
    # å¯ä»¥æ­£ç¡®çš„ç”Ÿæˆembeddingç»“æœ
    collection = chroma_client.get_or_create_collection(name="my_collection", embedding_function=sparkmodel)
    # ä¸ºä»€ä¹ˆæ˜¯None
    collection.add(
        documents=["This is a document", "cc", "1122"],
        metadatas=[{"source": "my_source"}, {"source": "my_source"}, {"source": "my_source"}],
        ids=["id1", "id2", "id3"]
    )
    # print(collection.peek())  #æ˜¾ç¤ºå‰äº”æ¡æ•°æ®
    print(collection.count())  # æ•°æ®åº“ä¸­æ•°æ®é‡
    results = collection.query(
        query_texts=["ac", 'documents'],
        n_results=2
    )
    print(results)  # æŸ¥è¯¢ç»“æœ


if __name__ == "__main__":
    test_embedding()
    test_chroma_embedding()
```

### å…¶å®ƒå‚æ•°

#### Parameterå‚æ•°

å¯é€šè¿‡æ„é€  `model_kwargs` å­—å…¸ä¼ å…¥Clienté…ç½®ä¸­å³å¯
```python
spark = ChatSparkLLM(
    spark_api_url="wss://xingchen-api.cn-huabei-1.xf-yun.com/v1.1/chat",
    spark_app_id=os.environ["SPARKAI_APP_ID"],
    spark_api_key=os.environ["SPARKAI_API_KEY"],
    spark_api_secret=os.environ["SPARKAI_API_SECRET"],
    spark_llm_domain="xspark13b6k",
    streaming=True,
    max_tokens=1024,
    model_kwargs={"search_disable": False},
)

```

#### patch_id

å¯é€šè¿‡æ„é€  `model_kwargs` å­—å…¸ä¼ å…¥`patch_id`å³å¯ï¼Œé€‚ç”¨äºloraæˆ–å°åŒ…ç±»å‹ç±»å‹æ¨ç†

```python
async def test_13b_lora():
    from sparkai.log.logger import logger
    logger.setLevel("debug")
    from sparkai.core.callbacks import StdOutCallbackHandler
    messages = [{'role': 'user',
                 'content': "å§æ§½"}]
    spark = ChatSparkLLM(
        spark_api_url="wss://xingchen-api.cn-huabei-1.xf-yun.com/v1.1/chat",
        spark_app_id=os.environ["SPARKAI_APP_ID"],
        spark_api_key=os.environ["SPARKAI_API_KEY"],
        spark_api_secret=os.environ["SPARKAI_API_SECRET"],
        spark_llm_domain="xspark13b6k",
        streaming=True,
        max_tokens=1024,
        model_kwargs={"patch_id": "210267877777408"},
    )
    messages = [
                ChatMessage(
                        role="user",
                        content=messages[0]['content']

    )]
    handler = AsyncChunkPrintHandler()
    a = spark.astream(messages, config={"callbacks": [handler]})
    async for message in a:
        print(message)
```

### è°ƒè¯•æ¨¡å¼

è®¾ç½®æ—¥å¿—çº§åˆ«:
```python
from sparkai.log.logger import logger
logger.setLevel("debug")
```


## æ¬¢è¿è´¡çŒ®

æ‰«ç åŠ å…¥äº¤æµç¾¤

![img](weichat.jpg)

## å·²çŸ¥é—®é¢˜

* é¡¹ç›®ç›®å‰å¼€å‘é˜¶æ®µï¼Œæœ‰ä¸€äº›å†—ä½™ä»£ç ï¼ŒäººåŠ›æœ‰é™ï¼Œéƒ¨åˆ†æ€æƒ³å€Ÿé‰´å¼€æºå®ç°
* Clientå½“å‰ä¸æ”¯æŒå¤šè·¯å¤ç”¨ï¼Œå¤šçº¿ç¨‹ä½¿ç”¨æ—¶ï¼Œ æ¯ä¸ªçº¿ç¨‹éœ€è¦å•ç‹¬å®ä¾‹åŒ–Client
* å½“å‰æµå¼æ¥å£ä¸æ”¯æŒæ¯å¸§ç»Ÿè®¡tokenæ•°é‡ï¼Œéœ€è¦sparkapiæ­£å¼æ”¯æŒè¯¥ç‰¹æ€§åï¼Œ sdkä¼šåŒæ­¥æ”¯æŒã€‚
* 

## URL

* wss://spark-api.xf-yun.com/v3.5/chat


## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=iflytek/spark-ai-python&type=Date)](https://star-history.com/#iflytek/spark-ai-python&Date)

## è‡´è°¢

* [Langchain Community](https://github.com/hwchase17/langchain)
